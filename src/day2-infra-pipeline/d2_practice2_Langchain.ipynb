{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1B5ID-QJGln"
   },
   "source": [
    "# Langchain\n",
    "- 대규모 언어 모델(LLM) 기반 애플리케이션 개발을 위한 프레임워크\n",
    "- 모델을 API를 통해 호출하는 것 뿐만 아니라 외부 데이터를 인식하거나, 타 시스템과 상호작용하는 애플리케이션 개발이 가능\n",
    "1. 실시간 데이터 보강 (Real-time data augmentation)\n",
    "  - LLM을 다양한 데이터 소스나 내·외부 시스템에 손쉽게 연결\n",
    "  - 모델 제공업체, 각종 도구, 벡터 스토어, 리트리버(검색 도구) 등의 방대한 통합 라이브러리\n",
    "2. 모델 상호 운용성 (Model interoperability)\n",
    "  - 간단하게 모델 교체 가능\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgG6UMKKJ8Qc"
   },
   "source": [
    "랭체인(LangChain) 프레임워크는 LLM 애플리케이션 개발에 도움이 되는 여러 구성 요소로 이루어져 있습니다.  \n",
    "특히 개발자들이 다양한 LLM 작업을 신속하게 구축하고 배포할 수 있도록 설계되었습니다.  \n",
    "랭체인의 주요 구성 요소는 다음과 같습니다.\n",
    "\n",
    "1. 랭체인 라이브러리(LangChain Libraries)\n",
    "  - 파이썬과 자바스크립트 라이브러리를 포함하며, 다양한 컴포넌트의 인터페이스와 통합, 이 컴포넌트들을 체인과 에이전트로 결합할 수 있는 기본 런타임, 그리고 체인과 에이전트의 사용 가능한 구현이 가능합니다.\n",
    "\n",
    "2. 랭체인 템플릿(LangChain Templates)\n",
    "  - 다양한 작업을 위한 쉽게 배포할 수 있는 참조 아키텍처 모음입니다. 이 템플릿은 개발자들이 특정 작업에 맞춰 빠르게 애플리케이션을 구축할 수 있도록 돕습니다.\n",
    "\n",
    "3. 랭서브(LangServe)\n",
    " - 랭체인 체인을 REST API로 배포할 수 있게 하는 라이브러리입니다. 이를 통해 개발자들은 자신의 애플리케이션을 외부 시스템과 쉽게 통합할 수 있습니다.\n",
    "\n",
    "4. 랭스미스(LangSmith)\n",
    "  - 개발자 플랫폼으로, LLM 프레임워크에서 구축된 체인을 디버깅, 테스트, 평가, 모니터링할 수 있으며, 랭체인과의 원활한 통합을 지원합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y74zYd6tKTRb"
   },
   "source": [
    "## 1. 필수 라이브러리 설치\n",
    "LangChain을 설치하면 langchain-core, langchain-community, langsmith 등이 함께 설치되어 프로젝트 수행에 필수적인 라이브러리들은 한번에 설치됩니다.  \n",
    "다만, 최소한의 기본적인 요구 사항만 충족되는 것이고, 다양한 외부 모델 제공자와 데이터 저장소 등과의 통합을 위해서는 개별적으로 의존성 설치가 필요합니다.  \n",
    "예를 들면, OpenAI에서 제공하는 LLM을 사용하려면 langchain-openai 의존성 라이브러리를 설치해야 합니다.  \n",
    "실습에서 OpenAI LLM을 사용할 것이기 때문에 `langchain-openai`와 `tiktoken` 설치가 필요합니다.   \n",
    "본 실습 환경에는 미리 설치가 되어 있으니 import 하여 사용하시면 됩니다.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 키가 설정되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드, 환경 변수에서 API 키 읽기\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdgrjKwZLXGI"
   },
   "source": [
    "## 2. 기본 LLM 체인 (Prompt+LLM)\n",
    "우리가 ChatGPT 같은 LLM에게 프롬프트를 넣고 답을 얻는 것처럼 프롬프트를 LLM 모델에게 전달하고 답변을 반환받는 과정을 langchain으로 구현해봅시다.  \n",
    "`invoke()`는 입력에 대해 체인을 호출하는 함수입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3FcHnaiyLuGt",
    "outputId": "f2b01b67-9f12-48ad-c110-5c6eb943152c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"strawberry\"에는 \\'r\\'이 2개 들어 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 19, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8zKV7uZopfax2LujE6PrLKQ79VfK', 'finish_reason': 'stop', 'logprobs': None}, id='run-f0e25d5e-9d23-4f0a-9ec9-7238e6d21df8-0', usage_metadata={'input_tokens': 19, 'output_tokens': 16, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# chain 실행\n",
    "llm.invoke(\"strawberry 에 r이 몇 개 들어있어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='9.11이 9.9보다 더 큽니다. 소수점 아래 숫자를 비교할 때, 9.11의 소수점 아래 첫 번째 자리인 1이 9.9의 소수점 아래 첫 번째 자리인 9보다 작기 때문에 9.11이 더 큰 숫자입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 23, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8zKYOHZPYjndNZZTy9FLAT06JDOi', 'finish_reason': 'stop', 'logprobs': None}, id='run-d1a137a9-8f9e-4c02-a6cb-610d6dad1aa9-0', usage_metadata={'input_tokens': 23, 'output_tokens': 70, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"9.11과 9.9 중에 어떤 숫자가 더 커?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m-YPHxGL6iE"
   },
   "source": [
    "### 프롬프트 템플릿 적용\n",
    "1일차에서 실습했던 여러가지 프롬프트 엔지니어링 기법을 이용하여 템플릿을 작성해봅시다.  \n",
    "이렇게 템플릿을 사용한 답변과 사용하지 않은 답변을 비교해봅시다.    \n",
    "`ChatPromptTemplate.from_template()` 메소드는 문자열 형태의 템플릿을 인자로 받아, 해당 형식에 맞는 프롬프트 객체를 생성합니다.  \n",
    "예제에서는 'Self Consistency' 기법을 사용하므로 템플릿 문자열은 다음과 같이 입력합니다.  \n",
    "`\"Show the solution step by step and clearly present the final answer. <Question>: {input}\"`  \n",
    "`<Question>: {input}` 부분에서 실제 질문을 받아 답변하도록 요청합니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9Ogqt1CN75R",
    "outputId": "c9b96fe2-c894-4525-fd5e-7cc27de1eb2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Show the solution step by step and clearly present the final answer. <Question>: {input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Show the solution step by step and clearly present the final answer. <Question>: {input}\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXJH-jTxOKPd"
   },
   "source": [
    "### Prompt + LLM : Chain 연결 (LCEL)\n",
    "위에서 우리가 만든 `prompt` 객체의 `<Question>: {input}` 부분의 `{input}`에 처음에 했던 질문이 들어가야 프롬프트가 완성됩니다.  \n",
    "이렇게 chain을 구성 하는 방법 중 하나로 `LCEL`이 있습니다.  \n",
    "`LCEL`은 langCHain Expression Language의 약자입니다.  \n",
    "LCEL은 LangChain의 모든 컴포넌트(Model, Prompt, Parser, Retriever 등)를 Runnable 이라는 표준 인터페이스로 취급합니다.  \n",
    "Runnable을 파이프(|) 연산자를 통해 연결할 수 있습니다.  \n",
    "마치 리눅스의 파이프라인과 같습니다.  \n",
    "이렇게 연결되어 완성된 프롬프트는 LLM에 전달되어, 모델이 입력된 질문에 대한 답변을 생성하게 됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v88HHp3aPZc-",
    "outputId": "f918a861-e9f0-4718-b2be-7d3b88f3f30a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='단어 \"strawberry\"에는 모음과 자음이 포함되어 있습니다. 여기에 있는 모든 \"r\"의 개수를 세어보겠습니다.\\n\\n1. **단어 확인**: strawberry\\n2. **r 찾기**: \\n   - 첫 번째 \"r\": strawberry\\n   - 두 번째 \"r\": strawb**r**y\\n\\n\"strawberry\"에는 2개의 \"r\"이 포함되어 있습니다.\\n\\n**최종 답변**: strawberry에는 r이 2개 들어있다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 33, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8zKfGEQ77u4v0SBkaM3XwYQPttgl', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe73b445-bbef-43a3-9084-c87c4a97af27-0', usage_metadata={'input_tokens': 33, 'output_tokens': 110, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "\n",
    "# chain 호출\n",
    "chain.invoke({\"input\": \"strawberry 에 r이 몇 개 들어있어?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjXpBeltPofc"
   },
   "source": [
    "### StrOutputParser\n",
    "현재 `llm.invoke()`의 결과가 AIMessage 객체이기 때문에 메타데이터까지 섞이면서 가독성이 좋지 않아 답변 비교가 어렵습니다.  \n",
    "LangChain이 제공하는 Output Parser 중 `SreOutputParser`는 AIMessage를 순수 문자열로 자동 변환해 줍니다.  \n",
    "모델과 파서를 파이프(|)로 연결하면, 답변이 문자열로 반환됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "XNwQiS9hRV4J",
    "outputId": "b1fe83b3-70f1-4f90-d008-e0a7752f3ed7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"strawberry\"라는 단어에 포함된 \\'r\\'의 개수를 세어보겠습니다.\\n\\n1. **단어 확인**: 단어는 \"strawberry\"입니다.\\n2. **각 글자 확인**: \\n   - s\\n   - t\\n   - r (1개)\\n   - a\\n   - w\\n   - b\\n   - e\\n   - r (2개)\\n   - r (3개)\\n   - y\\n\\n3. **r의 개수 세기**: \\'r\\'이 나타나는 개수를 세겠습니다.\\n   - 첫 번째 \\'r\\' \\n   - 두 번째 \\'r\\' \\n   - 세 번째 \\'r\\' \\n\\n결과적으로, \"strawberry\"에는 \\'r\\'이 **3개** 포함되어 있습니다.\\n\\n최종 답변: strawberry에는 r이 **3개** 들어있습니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# prompt + model + output parser\n",
    "prompt = ChatPromptTemplate.from_template(\"Show the solution step by step and clearly present the final answer. <Question>: {input}\")\n",
    "llm   = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL chaining\n",
    "chain = prompt | llm | output_parser            # 프롬프트 ↦ 모델 ↦ 파서 체인\n",
    "\n",
    "# chain 호출\n",
    "chain.invoke({\"input\": \"strawberry 에 r이 몇 개 들어있어?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCUX971OSKBu"
   },
   "source": [
    "답변을 보면 AIMessage(content='') 형식에서 문자열만 남은 걸 볼 수 있습니다.  \n",
    "GPT 계열 모델은 웹 인터페이스에서 바로 보이도록 Markdown으로 응답하도록 학습이 되어 있으므로 Text Cell에 복사 붙여넣기하여 답변을 확인해보면 결과가 잘 나온 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDullaalSGuD"
   },
   "source": [
    "\"Strawberry\"라는 단어에 포함된 \"r\"의 개수를 세어 보겠습니다.\\n\\n1. \"strawberry\"라는 단어를 적어봅니다:\\n   - \"strawberry\"\\n\\n2. 단어를 한 글자씩 나누어서 \"r\"이 몇 번 나타나는지 셉니다:\\n   - s - t - r - a - w - b - e - r - r - y\\n\\n3. 위의 글자들 중에서 \"r\"을 찾아봅니다:\\n   - 첫 번째 \"r\" (3번째 글자)\\n   - 두 번째 \"r\" (8번째 글자)\\n   - 세 번째 \"r\" (9번째 글자)\\n\\n4. 총 세 개의 \"r\"이 존재하는 것을 확인했습니다.\\n\\n결론적으로, \"strawberry\"라는 단어에는 \"r\"이 **3개** 들어있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDalezpGUqd-"
   },
   "source": [
    "## 실습\n",
    "`OutputParser`에는 `StrOputputParser`외에도 출력을 JSON 형식으로 파싱하는 `JsonOutputParser`, 출력을 Pydantic 모델로 변환하는 `PydanticOutputParser`, 쉼표로 구분된 리스트로 파싱하는 `CommaSeparatedListOutputParser`도 있습니다.  \n",
    "Input과 Prompt Template, OutputParser를 바꿔가며 답변을 직접 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wqvl6XtsU1SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'인물': '김갑수', '장소': '서울역', '시간': '내일 오후 3시'}\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
    "\n",
    "json_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "문장에서 '인물', '장소', '시간' 정보를 추출하여 JSON으로 출력해줘.\n",
    "'인물' 이름에서 호칭은 떼고 순 이름만 저장해.\n",
    "문장: {sentance}\n",
    "JSON 출력:\n",
    "\"\"\")\n",
    "json_chain = json_prompt | llm | JsonOutputParser()\n",
    "json_out = json_chain.invoke({\"sentance\": \"내일 오후 3시에 서울역에서 김갑수씨를 만나기로 했어요.\"})\n",
    "print(json_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location='서울시 강남구 개포3동 SH 2층 대강당' time='25년 8월27일 오후 3ㅣ' attendee=['김상성', '이전자', '윤수일']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Parser class\n",
    "class EmailParser(BaseModel):\n",
    "    location: Optional[str] = Field(description=\"회의가 열리는 장소\")\n",
    "    time: Optional[str] = Field(description=\"회의 시작 시간\")\n",
    "    attendee: List[str] = Field(description=\"회의 참석자들의 이름 목록\", default=[])\n",
    "        \n",
    "# 인스턴스\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=EmailParser)\n",
    "\n",
    "# Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "주어진 이메일을 읽고 회의 정보를 추출해\n",
    "email: {email_body}\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "chain = prompt | llm | pydantic_parser\n",
    "sample_email = \"\"\"\n",
    "하이, 담당자님\n",
    "다음주 프로젝트 회의 일정을 공유합니다.\n",
    "- 일시 : 25년 8월27일 오후 3ㅣ\n",
    "- 장소 : 서울시 강남구 개포3동 SH 2층 대강당\n",
    "- 참석자: 김상성, 이전자, 윤수일\n",
    "감사합니다.\n",
    "\"\"\"\n",
    "\n",
    "pyd_out = chain.invoke({\"email_body\": sample_email,                        \n",
    "                       \"format_instructions\": pydantic_parser.get_format_instructions()})\n",
    "print(pyd_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z5APouKXDp7"
   },
   "source": [
    "## 3. Runnable 프로토콜\n",
    "LangChain의 `Runnable` 프로토콜은 사용자가 사용자 정의 체인을 쉽게 생성하고 관리할 수 있도록 설계된 핵심적인 개념입니다.  \n",
    "이 프로토콜을 통해, 일관된 인터페이스를 사용하여 다양한 타입의 컴포넌트를 조합하고, 복잡한 데이터 처리 파이프라인을 구성할 수 있습니다.  \n",
    "  \n",
    "`Runnable` 프로토콜 주요 메서드는 다음과 같습니다.  \n",
    "- `invoke`\n",
    "  - 주어진 입력에 대해 체인을 호출하고, 결과를 반환\n",
    "  - 단일 입력에 대해 동기적으로 작동\n",
    "\n",
    "- `batch`\n",
    "  - 입력 리스트에 대해 체인을 호출하고, 각 입력에 대한 결과를 리스트로 반환\n",
    "  - 여러 입력에 대해 동기적으로 작동하며, 효율적인 배치 처리를 가능하게 함\n",
    "\n",
    "- `stream`\n",
    "  - 입력에 대해 체인을 호출하고, 결과를 스트리밍    \n",
    "  - 대용량 데이터 처리나 실시간 데이터 처리에 유용  \n",
    "\n",
    "- 비동기\n",
    "  - 각 메서드의 앞에 'a'을 붙이면 비동기 메서드\n",
    "  - `ainvoke`, `abatch`, `astream`\n",
    "  \n",
    "LangChain의 `Runnable` 프로토콜을 사용하면, 보다 유연하고 확장 가능한 방식으로 데이터 처리 작업을 설계하고 구현할 수 있으며, 복잡한 언어 처리 작업을 보다 쉽게 관리할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8C9MpXHZetB"
   },
   "source": [
    "### invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJDsSawKZhzU",
    "outputId": "e105d4ae-4c02-4712-ae48-bda1f91aef2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke 결과: 지구 자전은 지구가 자전 축을 중심으로 하루에 한 바퀴 도는 운동을 말합니다. 이 자전은 서쪽에서 동쪽으로 진행되며, 지구의 자전 주기는 약 24시간입니다. 이 자전 운동으로 인해 낮과 밤이 발생하게 되며, 지구의 자전 속도는 적도에서 약 시속 1,670킬로미터에 달합니다.\n",
      "\n",
      "지구 자전은 여러 가지 중요한 현상에 영향을 미칩니다. 예를 들어, 코리올리 효과는 지구 자전에 의해 발생하며, 이는 대기와 해양의 흐름에 영향을 미치는 중요한 요소입니다. 또한, 자전 운동은 주기적인 계절 변화와 같은 자연 현상에도 영향을 미칩니다. 지구의 자전은 또한 지구의 형태에 영향을 미쳐, 적도 부근이 부풀어 오르고 극지방이 수축하는 현상을 만들어 냅니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# 1. 컴포넌트 정의\n",
    "prompt = ChatPromptTemplate.from_template(\"지구과학에서 {topic}에 대해 간단히 설명해주세요.\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 2. 체인 생성\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 3. invoke 메소드 사용\n",
    "result = chain.invoke({\"topic\": \"지구 자전\"})\n",
    "print(\"invoke 결과:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjhtDSrFZvXa"
   },
   "source": [
    "### batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__xv9P_AZyC1",
    "outputId": "f1712457-6deb-4fa2-98a4-8a36a997dca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지구 공전 설명: 지구 공전은 지구가 태양 주위를 한 바퀴 도는 운동을 말합니다. 지구는 약 365.25일을...\n",
      "화산 활동 설명: 화산 활동은 지구 내부의 마그마가 지표로 발산하는 현상을 말합니다. 화산은 마그마가 지구 ...\n",
      "대륙 이동 설명: 대륙 이동이론은 지구의 표면이 여러 개의 큰 판으로 나누어져 있으며, 이들이 서서히 이동한...\n"
     ]
    }
   ],
   "source": [
    "# batch 메소드 사용\n",
    "topics = [\"지구 공전\", \"화산 활동\", \"대륙 이동\"]\n",
    "results = chain.batch([{\"topic\": t} for t in topics])\n",
    "for topic, result in zip(topics, results):\n",
    "    print(f\"{topic} 설명: {result[:50]}...\")  # 결과의 처음 50자만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZqcpFwrZ1Qh"
   },
   "source": [
    "### stream()\n",
    "주피터 노트북이나 코랩에서는 이미 내부적으로 이벤트 루프(asyncio)가 돌고 있습니다.  \n",
    "따라서 현재 돌아가는 이벤트 루프 위에 중첩 실행이 가능하도록 `nest_asyncio.apply()`로 패치하여 `asyncio.run()`이 문제없이 실행되도록 합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97g5aSftZ4op",
    "outputId": "1c48e3ac-510c-404d-953a-61e35c89ce5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ainvoke 결과: 해류는 바다에서 물이 일정한 방향으로 흐르는 현상을 말합니다. 이러한 해류는 여러 가지 요 ...\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "# nest_asyncio 적용 (구글 코랩 등 주피터 노트북에서 실행 필요)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 비동기 메소드 사용 (async/await 구문 필요)\n",
    "async def run_async():\n",
    "    result = await chain.ainvoke({\"topic\": \"해류\"})\n",
    "    print(\"ainvoke 결과:\", result[:50], \"...\")\n",
    "\n",
    "asyncio.run(run_async())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xjCXqGCbDbH"
   },
   "source": [
    "## Ollama\n",
    "\n",
    "Ollama는 로컬 환경에서 대규모 언어 모델을 쉽게 실행할 수 있는 오픈소스 도구입니다. \n",
    "LangChain과 연동하여 로컬 LLM을 사용할 수 있으며, API 키 없이도 모델을 실행할 수 있습니다.\n",
    "\n",
    "### 주요 특징:\n",
    "- 로컬 실행으로 데이터 프라이버시 보장\n",
    "- 다양한 오픈소스 모델 지원\n",
    "- 간편한 설치와 사용\n",
    "- REST API 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hcOVoON_0YLy"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sv0nYaLzbY5X",
    "outputId": "e06ab61c-ebdc-4fe6-a912-e7ffe8f9846c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   0% ▕                  ▏ 3.6 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   2% ▕                  ▏  40 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   5% ▕                  ▏  86 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   5% ▕                  ▏  86 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   5% ▕                  ▏  86 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   5% ▕                  ▏  86 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   5% ▕                  ▏  86 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   5% ▕                  ▏  86 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   5% ▕                  ▏  89 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   6% ▕█                 ▏ 100 MB/1.7 GB                  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:   9% ▕█                 ▏ 149 MB/1.7 GB  145 MB/s     10s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  12% ▕██                ▏ 197 MB/1.7 GB  145 MB/s     10s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  13% ▕██                ▏ 221 MB/1.7 GB  145 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  14% ▕██                ▏ 241 MB/1.7 GB  145 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  14% ▕██                ▏ 241 MB/1.7 GB  145 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  14% ▕██                ▏ 241 MB/1.7 GB  145 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  14% ▕██                ▏ 241 MB/1.7 GB  145 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  14% ▕██                ▏ 241 MB/1.7 GB  145 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  14% ▕██                ▏ 241 MB/1.7 GB  145 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  15% ▕██                ▏ 258 MB/1.7 GB  145 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  18% ▕███               ▏ 303 MB/1.7 GB  149 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  19% ▕███               ▏ 326 MB/1.7 GB  149 MB/s      9s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  22% ▕████              ▏ 375 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  25% ▕████              ▏ 416 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  25% ▕████              ▏ 416 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  25% ▕████              ▏ 416 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  25% ▕████              ▏ 416 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  25% ▕████              ▏ 416 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  25% ▕████              ▏ 416 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  25% ▕████              ▏ 427 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  27% ▕████              ▏ 451 MB/1.7 GB  149 MB/s      8s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  30% ▕█████             ▏ 504 MB/1.7 GB  158 MB/s      7s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  33% ▕█████             ▏ 554 MB/1.7 GB  158 MB/s      7s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  35% ▕██████            ▏ 582 MB/1.7 GB  158 MB/s      6s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  36% ▕██████            ▏ 596 MB/1.7 GB  158 MB/s      6s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  36% ▕██████            ▏ 596 MB/1.7 GB  158 MB/s      6s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  36% ▕██████            ▏ 596 MB/1.7 GB  158 MB/s      6s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  36% ▕██████            ▏ 596 MB/1.7 GB  158 MB/s      6s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  36% ▕██████            ▏ 596 MB/1.7 GB  158 MB/s      6s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  36% ▕██████            ▏ 600 MB/1.7 GB  158 MB/s      6s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  38% ▕██████            ▏ 642 MB/1.7 GB  158 MB/s      6s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  41% ▕███████           ▏ 693 MB/1.7 GB  166 MB/s      5s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  43% ▕███████           ▏ 720 MB/1.7 GB  166 MB/s      5s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling c1864a5eb193:  46% ▕████████          ▏ 771 MB/1.7 GB  166 MB/s      5s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling c1864a5eb193:  49% ▕████████          ▏ 825 MB/1.7 GB  166 MB/s      5s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\r\n",
      "pulling c1864a5eb193:  49% ▕████████          ▏ 826 MB/1.7 GB  166 MB/s      5s\u001b[K\u001b[?25h\u001b[?2026l"
     ]
    }
   ],
   "source": [
    "# 모델 다운로드\n",
    "!ollama pull gemma:2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "id": "sWaZilD_cbCo",
    "outputId": "d439a5ad-67a4-43c5-9a0b-2df7a04313dd"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# model\n",
    "llm = ChatOllama(model=\"gemma:2b\")\n",
    "\n",
    "# chain 실행\n",
    "llm.invoke(\"지구의 자전 주기는?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치 및 병렬화를 통한 성능 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 처리\n",
    "\n",
    "배치 처리는 여러 요청을 하나의 API 호출로 묶어서 처리하는 방법입니다.\n",
    "\n",
    "#### 배치 처리의 특징:\n",
    "- **하나의 API 호출**로 여러 요청을 묶어서 처리\n",
    "- **순차적으로 내부 처리**하지만 외부에서는 하나의 요청\n",
    "- **메모리 효율적**: 한 번에 처리하므로 오버헤드 적음\n",
    "- **단순함**: 코드가 간단하고 이해하기 쉬움\n",
    "\n",
    "#### 동작 방식:\n",
    "사용자 → [요청1, 요청2, 요청3] → 모델 → [응답1, 응답2, 응답3] → 사용자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama 배치 처리 기본 실습\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 모델 및 체인 설정\n",
    "llm = ChatOllama(model=\"gemma:2b\")\n",
    "prompt = ChatPromptTemplate.from_template(\"지구과학에서 {topic}에 대해 간단히 설명해주세요.\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 배치 처리 실행\n",
    "topics = [\"지구 자전\", \"화산 활동\", \"대륙 이동\", \"해류\", \"지진\"]\n",
    "\n",
    "print(\"=== 배치 처리 실습 ===\")\n",
    "results = chain.batch([{\"topic\": topic} for topic in topics])\n",
    "\n",
    "for i, (topic, result) in enumerate(zip(topics, results), 1):\n",
    "    print(f\"{i}. 주제: {topic}\")\n",
    "    print(f\"   답변: {result[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 병렬 처리 (ThreadPoolExecutor)\n",
    "\n",
    "병렬 처리는 여러 스레드를 사용하여 요청을 동시에 처리하는 방법입니다.\n",
    "\n",
    "#### 병렬 처리의 특징:\n",
    "- **여러 스레드**가 동시에 각각의 요청을 처리\n",
    "- **실제 동시 실행**: CPU 코어를 여러 개 사용\n",
    "- **네트워크 병렬화**: 여러 API 호출이 동시에 발생\n",
    "- **I/O 대기 시간 최소화**: 한 요청이 대기하는 동안 다른 요청 처리\n",
    "\n",
    "#### 동작 방식:\n",
    "- 스레드1: 요청1 → 모델 → 응답1\n",
    "- 스레드2: 요청2 → 모델 → 응답2  (동시 실행)\n",
    "- 스레드3: 요청3 → 모델 → 응답3\n",
    "\n",
    "#### 성능:\n",
    "- 네트워크 지연이 큰 경우: 가장 빠름 (실제 동시 실행)    \n",
    "- CPU 집약적인 경우: 가장 빠름 (실제 멀티코어 활용)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 단일 요청 처리 함수\n",
    "def single_request(prompt):\n",
    "    \"\"\"단일 요청 처리\"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "def parallel_processing(prompts, max_workers=4):\n",
    "    \"\"\"ThreadPoolExecutor를 사용한 병렬 처리\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        start_time = time.time()\n",
    "        futures = [executor.submit(single_request, prompt) for prompt in prompts]\n",
    "        results = [future.result() for future in futures]\n",
    "        end_time = time.time()\n",
    "    return results, end_time - start_time\n",
    "\n",
    "# 병렬 처리 실행\n",
    "prompts = [\n",
    "    \"지구의 자전 주기는?\",\n",
    "    \"화산 활동의 원인은?\",\n",
    "    \"대륙 이동 이론이란?\",\n",
    "    \"해류의 역할은?\",\n",
    "    \"지진의 발생 원인은?\"\n",
    "]\n",
    "\n",
    "print(\"=== 병렬 처리 실습 (ThreadPoolExecutor) ===\")\n",
    "parallel_results, parallel_time = parallel_processing(prompts, max_workers=4)\n",
    "\n",
    "for i, (prompt, result) in enumerate(zip(prompts, parallel_results), 1):\n",
    "    print(f\"{i}. 질문: {prompt}\")\n",
    "    print(f\"   답변: {result.content[:100]}...\")  # .content 추가\n",
    "    print()\n",
    "\n",
    "print(f\"병렬 처리 시간: {parallel_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비동기 병렬 처리 (asyncio)\n",
    "\n",
    "비동기 병렬 처리는 단일 스레드에서 이벤트 루프를 사용하여 논리적으로 동시에 처리하는 방법입니다.\n",
    "\n",
    "#### 비동기 병렬 처리의 특징:\n",
    "- **단일 스레드**에서 **이벤트 루프**를 사용한 비동기 처리\n",
    "- **논리적 동시성**: 실제로는 번갈아가며 실행하지만 동시에 실행되는 것처럼 보임\n",
    "- **메모리 효율적**: 스레드 생성 오버헤드 없음\n",
    "- **I/O 바운드 작업에 최적**: 네트워크 대기 시간을 효율적으로 활용\n",
    "\n",
    "#### 동작 방식:\n",
    "이벤트 루프: 요청1 → 대기 → 요청2 → 대기 → 요청3 → 대기\n",
    "                    ↓ 응답1    ↓ 응답2    ↓ 응답3\n",
    "\n",
    "#### 성능:\n",
    "- 네트워크 지연이 큰 경우: 빠름 (논리적 동시 실행)\n",
    "- CPU 집약적인 경우: 느림 (단일 스레드 제약)\n",
    "- 메모리 사용량: 효율적 (스레드 오버헤드 없음)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def async_single_request(prompt):\n",
    "    \"\"\"비동기 단일 요청\"\"\"\n",
    "    return await llm.ainvoke(prompt)\n",
    "\n",
    "async def async_parallel_processing(prompts):\n",
    "    \"\"\"asyncio를 사용한 비동기 병렬 처리\"\"\"\n",
    "    start_time = time.time()\n",
    "    tasks = [async_single_request(prompt) for prompt in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "    return results, end_time - start_time\n",
    "\n",
    "# 비동기 병렬 처리 실행\n",
    "async def run_async_parallel():\n",
    "    prompts = [\n",
    "        \"지구의 자전 주기는?\",\n",
    "        \"화산 활동의 원인은?\",\n",
    "        \"대륙 이동 이론이란?\",\n",
    "        \"해류의 역할은?\",\n",
    "        \"지진의 발생 원인은?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== 비동기 병렬 처리 실습 (asyncio) ===\")\n",
    "    parallel_results, parallel_time = await async_parallel_processing(prompts)\n",
    "    \n",
    "    for i, (prompt, result) in enumerate(zip(prompts, parallel_results), 1):\n",
    "        print(f\"{i}. 질문: {prompt}\")\n",
    "        print(f\"   답변: {result.content[:100]}...\")  # .content 추가\n",
    "        print()\n",
    "    \n",
    "    print(f\"비동기 병렬 처리 시간: {parallel_time:.2f}초\")\n",
    "\n",
    "# 실행\n",
    "asyncio.run(run_async_parallel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능 비교 실습\n",
    "\n",
    "네 가지 처리 방법의 성능을 비교하여 각각의 장단점을 확인해보겠습니다.\n",
    "\n",
    "#### 비교 대상:\n",
    "1. **순차 처리**: 요청을 하나씩 순서대로 처리\n",
    "2. **배치 처리**: 여러 요청을 하나의 API 호출로 묶어서 처리\n",
    "3. **병렬 처리**: ThreadPoolExecutor를 사용한 멀티스레드 처리\n",
    "4. **비동기 병렬 처리**: asyncio를 사용한 비동기 처리\n",
    "\n",
    "#### 성능 비교 기준:\n",
    "- **처리 시간**: 전체 요청 처리에 걸리는 시간\n",
    "- **성능 향상**: 순차 처리 대비 개선 정도\n",
    "- **메모리 사용량**: 처리 과정에서 사용되는 메모리\n",
    "\n",
    "#### 선택 가이드:\n",
    "- **간단한 처리**: 배치 처리\n",
    "- **네트워크 지연이 큰 경우**: 병렬 처리\n",
    "- **메모리가 제한적인 경우**: 비동기 병렬 처리\n",
    "- **최대 성능이 필요한 경우**: 병렬 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compare_all_methods():\n",
    "    prompts = [\n",
    "        \"지구의 자전 주기는?\",\n",
    "        \"화산 활동의 원인은?\",\n",
    "        \"대륙 이동 이론이란?\",\n",
    "        \"해류의 역할은?\",\n",
    "        \"지진의 발생 원인은?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== 성능 비교 실습 ===\")\n",
    "    \n",
    "    # 1. 순차 처리\n",
    "    start_time = time.time()\n",
    "    sequential_results = [single_request(prompt) for prompt in prompts]\n",
    "    sequential_time = time.time() - start_time\n",
    "    \n",
    "    # 2. 배치 처리\n",
    "    start_time = time.time()\n",
    "    batch_results = llm.batch(prompts)\n",
    "    batch_time = time.time() - start_time\n",
    "    \n",
    "    # 3. 병렬 처리\n",
    "    parallel_results, parallel_time = parallel_processing(prompts, max_workers=4)\n",
    "    \n",
    "    # 4. 비동기 병렬 처리\n",
    "    async_start_time = time.time()\n",
    "    async_parallel_results, async_parallel_time = await async_parallel_processing(prompts)\n",
    "    \n",
    "    print(f\"순차 처리: {sequential_time:.2f}초\")\n",
    "    print(f\"배치 처리: {batch_time:.2f}초\")\n",
    "    print(f\"병렬 처리: {parallel_time:.2f}초\")\n",
    "    print(f\"비동기 병렬 처리: {async_parallel_time:.2f}초\")\n",
    "    \n",
    "    print(f\"\\n=== 성능 향상 ===\")\n",
    "    print(f\"배치 vs 순차: {sequential_time/batch_time:.1f}배 향상\")\n",
    "    print(f\"병렬 vs 순차: {sequential_time/parallel_time:.1f}배 향상\")\n",
    "    print(f\"비동기 병렬 vs 순차: {sequential_time/async_parallel_time:.1f}배 향상\")\n",
    "    \n",
    "    # 최적 방법 찾기\n",
    "    methods = [\n",
    "        (\"순차\", sequential_time),\n",
    "        (\"배치\", batch_time),\n",
    "        (\"병렬\", parallel_time),\n",
    "        (\"비동기 병렬\", async_parallel_time)\n",
    "    ]\n",
    "    best_method = min(methods, key=lambda x: x[1])\n",
    "    print(f\"\\n최적 방법: {best_method[0]} 처리\")\n",
    "\n",
    "# 실행\n",
    "asyncio.run(compare_all_methods())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 크기 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 크기 최적화 실습\n",
    "batch_sizes = [1, 3, 5, 10]\n",
    "performance_results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    start_time = time.time()\n",
    "    test_topics = [f\"주제{i}\" for i in range(batch_size)]\n",
    "    results = chain.batch([{\"topic\": topic} for topic in test_topics])\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time_per_item = (end_time - start_time) / batch_size\n",
    "    performance_results.append((batch_size, avg_time_per_item))\n",
    "    print(f\"배치 크기 {batch_size}: 평균 {avg_time_per_item:.2f}초/항목\")\n",
    "\n",
    "# 최적 배치 크기 찾기\n",
    "optimal_batch_size = min(performance_results, key=lambda x: x[1])\n",
    "print(f\"\\n최적 배치 크기: {optimal_batch_size[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVUYgB4u0YLz"
   },
   "source": [
    "### 실습\n",
    "OpenAI API를 이용해서 langchain의 기본적인 사용법을 익히고,  \n",
    "Ollama를 이용하여 로컬 LLM을 사용하는 방법까지 익혔습니다.  \n",
    "지금까지 배운 내용을 바탕으로 Ollama 에서 지원하는 다른 모델을 다운받고  \n",
    "prompt template, output parser, 메서드를 바꿔가며 원하는 답을 얻어보세요.  \n",
    "지원하는 모델은 올라마 웹사이트에서 확인할 수 있습니다.  \n",
    "모델명을 잘 확인하세요 !   \n",
    "https://ollama.com/search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc1Xno230YLz"
   },
   "source": [
    "#### 📌 주의\n",
    "새로운 모델을 다운받을 때 `ollama run <모델 이름>` 이 아니라 `ollama pull <모델 이름>`을 사용해야 합니다.\n",
    "`ollama pull <모델 이름>` : 모델 가중치만 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyCK2nrF0YLz"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
