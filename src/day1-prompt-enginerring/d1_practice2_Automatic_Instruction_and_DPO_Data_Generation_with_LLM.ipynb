{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMPTFCpU1fUY"
   },
   "source": [
    "# LLM 성능 극대화를 위한 합성 데이터 생성 실습\n",
    "\n",
    "## 실습 목표\n",
    "데이터 부족 문제를 해결하기 위해 고품질의 합성 데이터를 생성하는는 두 가지 방법을 직접 실습합니다:\n",
    "\n",
    "1. **명령어 미세 조정(SFT)** 데이터 생성\n",
    "   - 지시사항과 응답 쌍을 자동으로 생성\n",
    "   - 모델이 사용자 지시를 더 잘 따르도록 훈련\n",
    "\n",
    "2. **직접 선호 최적화(DPO)** 데이터 생성\n",
    "   - 좋은 답변과 나쁜 답변 쌍을 자동으로 생성\n",
    "   - 모델의 선호도를 인간 가치에 맞게 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAwV0c5Y45-3"
   },
   "source": [
    "### 사용 모델: Kakao Kanana 1.5 2.1B Instruct\n",
    "- **개발사**: Kakao\n",
    "- **모델 크기**: 2.1B 파라미터\n",
    "- **모델 타입**: Instruct (지시사항 특화)\n",
    "- **언어 지원**: 한국어 특화\n",
    "- **버전**: 2505 (2025년 5월 버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159,
     "referenced_widgets": [
      "5f47ea24d18b4ee0961999f233109282",
      "cdd0fa5a63ab461399c7aecd2a15d575",
      "16f2cb1766ce40b1bfeff42e1a1ce4ac",
      "0182693b19d240cd9d21b343784f9323",
      "bcf677fba02b4fd79d96271c17bdae35",
      "27c861d6e5454f77a5915f256f5fce58",
      "95a374b0370d493da512b8d9871582df",
      "568bb6c2aa5a482f91337aa2f9c38aba",
      "63753e9b3c144c5ab90ae3c8068dbaf1",
      "7c76958b6904458693276ca3b938a1c6",
      "dedc6a0898514be597789176ccece224"
     ]
    },
    "id": "BW-eVcdrKvlf",
    "outputId": "9025cea4-fd54-4006-ff64-f57e57606b79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kanana 모델 로딩 성공!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델 ID - Kakao Kanana 1.5 2.1B\n",
    "model_id = \"kakaocorp/kanana-1.5-2.1b-instruct-2505\"  # 또는 다른 가능한 ID\n",
    "\n",
    "# 토크나이저 로딩\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 패딩 토큰 설정\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 모델 로딩\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else \"auto\",\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Kanana 모델 로딩 성공!\")  # 약 7분 소요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HolbQHRj2NQr"
   },
   "source": [
    "## Instruction 데이터 자동 생성\n",
    "\n",
    "### 실습 방법\n",
    "특정 \"시드(seed) 컨텍스트\"를 바탕으로 LLM에게 질문(instruction)과 답변(output)을 생성하도록 하여 SFT 데이터를 자동으로 구축합니니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-_QNNzN2aQ5"
   },
   "source": [
    "### SEED 컨텍스트 정의\n",
    "데이터 생성을 위한 기반 지식이 될 텍스트입니다.   \n",
    "이 정보를 바탕으로 모델이 질문과 답변을 생성합니다.  \n",
    "이번 실습에서는는 '클로드 3.5 소네트'에 대한 최신 정보를 시드 컨텍스트로 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGLB79It1a9a"
   },
   "outputs": [],
   "source": [
    "# SFT 데이터 생성을 위한 기반 지식\n",
    "\n",
    "seed_context = \"\"\"\n",
    "앤트로픽이 최신 모델 '클로드 3.5 소네트'를 출시했습니다. 이 모델은 이전 주력 모델이었던 '클로드 3 오퍼스'보다 2배 빠른 속도를 자랑하며, 더 저렴한 비용으로 제공됩니다. '클로드 3.5 소네트'는 특히 추론, 코드 생성, 시각 정보 이해 능력에서 큰 발전을 이루었습니다.\n",
    "\n",
    "주요 특징 중 하나는 '아티팩트(Artifacts)'라는 새로운 기능입니다. 사용자가 클로드에게 코드 생성을 요청하면, 결과물이 별도의 창에 나타나 바로 실행하거나 편집할 수 있습니다. 이는 개발자들의 작업 흐름을 크게 개선할 수 있는 혁신적인 기능으로 평가받고 있습니다.\n",
    "\n",
    "또한, 비전(Vision) 성능이 뛰어나 차트나 그래프를 텍스트로 정확하게 변환하거나, 이미지 속 텍스트를 추출하는 능력이 이전 모델을 능가합니다. 앤트로픽은 '클로드 3.5 소네트'를 시작으로, 앞으로 '3.5 하이쿠'와 '3.5 오퍼스' 모델도 연내에 출시할 계획이라고 밝혔습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juG0_aab2wIY"
   },
   "source": [
    "### 데이터 생성을 위한 프롬프트 템플릿 설계\n",
    "\n",
    "#### 프롬프트 엔지니어링의 중요성\n",
    "LLM에게 원하는 출력 형식을 명확히 지정하는 것이 핵심입니다.\n",
    "\n",
    "#### 설계 원칙\n",
    "1. **명확성**: 모델이 이해하기 쉬운 지시사항\n",
    "2. **구체성**: 원하는 출력 형식을 명확히 지정\n",
    "3. **범용성**: 다양한 주제에 적용 가능\n",
    "4. **실용성**: 실제 훈련에 바로 사용 가능\n",
    "\n",
    "#### 프롬프트 구성 요소\n",
    "- **역할 정의**: 모델의 역할과 목적 명시\n",
    "- **요구사항**: 구체적인 생성 규칙\n",
    "- **출력 형식**: 원하는 결과 형태 지정\n",
    "- **예시**: 모델이 참고할 수 있는 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHVJj4Kb2wQf"
   },
   "outputs": [],
   "source": [
    "# SFT 데이터 생성을 위한 프롬프트 템플릿\n",
    "# LLM에게 원하는 출력 형식을 명확히 지정합니다.\n",
    "\n",
    "sft_prompt_template = f\"\"\"\n",
    "다음 정보를 바탕으로 AI 훈련용 지시 데이터를 생성해주세요:\n",
    "\n",
    "{seed_context}\n",
    "\n",
    "이 주제의 다양한 측면을 다루는 3-5개의 질문-답변 쌍을 생성해주세요.\n",
    "\n",
    "요구사항:\n",
    "- 질문은 사용자가 실제로 궁금해할 만한 것들\n",
    "- 답변은 포괄적이고 정확해야 함\n",
    "- 다양한 난이도 포함\n",
    "- 기본적인 질문과 고급 질문 모두 포함\n",
    "\n",
    "각 쌍의 형식:\n",
    "질문: [사용자 질문]\n",
    "답변: [상세한 응답]\n",
    "\n",
    "생성 시작:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MefRJs1_24jK"
   },
   "source": [
    "### SFT 데이터 생성 실행\n",
    "\n",
    "#### 생성 과정\n",
    "1. **토크나이징**: 프롬프트를 모델이 이해할 수 있는 형태로 변환\n",
    "2. **텍스트 생성**: 모델을 통한 자동 응답 생성\n",
    "3. **결과 디코딩**: 생성된 토큰을 읽기 쉬운 텍스트로 변환\n",
    "\n",
    "#### 생성 파라미터\n",
    "- **max_new_tokens**: 생성할 최대 토큰 수 (응답 길이 제어)\n",
    "- **temperature**: 창의성 vs 일관성 조절 (0.7 권장)\n",
    "- **do_sample**: 확률적 샘플링 사용 여부\n",
    "- **repetition_penalty**: 반복 방지 계수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저를 사용해 모델 입력 형식으로 변환\n",
    "inputs = tokenizer.encode(sft_prompt_template, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# 모델을 통해 텍스트 생성\n",
    "# max_new_tokens: 생성할 최대 토큰 수\n",
    "# do_sample=True: 좀 더 창의적인 답변을 위해 샘플링 사용\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=1500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "\n",
    "# 생성된 결과 디코딩\n",
    "response_text = tokenizer.decode(outputs[0][inputs.shape[-1] :], skip_special_tokens=True)\n",
    "\n",
    "print(\"생성된 SFT 데이터\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "prompt를 수정해보고 결과를 출력해 비교해보세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래는 기존에 사용했던 prompt 템플릿입니다.\n",
    "# 어떤 부분을 수정해야 더 나은 결과를 얻을 수 있을지 여러 실험을 통해 알아봅시다.\n",
    "\n",
    "sft_prompt_template = f\"\"\"\n",
    "다음 정보를 바탕으로 AI 훈련용 지시 데이터를 생성해주세요:\n",
    "\n",
    "{seed_context}\n",
    "\n",
    "이 주제의 다양한 측면을 다루는 3-5개의 질문-답변 쌍을 생성해주세요.\n",
    "\n",
    "요구사항:\n",
    "- 질문은 사용자가 실제로 궁금해할 만한 것들\n",
    "- 답변은 포괄적이고 정확해야 함\n",
    "- 다양한 난이도 포함\n",
    "- 기본적인 질문과 고급 질문 모두 포함\n",
    "\n",
    "각 쌍의 형식:\n",
    "질문: [사용자 질문]\n",
    "답변: [상세한 응답]\n",
    "\n",
    "생성 시작:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(sft_prompt_template, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "\n",
    "response_text = tokenizer.decode(outputs[0][inputs.shape[-1] :], skip_special_tokens=True)\n",
    "\n",
    "print(\"생성된 SFT 데이터\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f994g-333HMm"
   },
   "source": [
    "## 선호도(DPO) 데이터 자동 생성\n",
    "\n",
    "### DPO(Direct Preference Optimization)란?\n",
    "DPO는 '더 나은 응답(chosen)'과 '덜 나은 응답(rejected)' 쌍을 학습하여 모델의 답변 품질을 직접적으로 향상시키는 기법입니다.\n",
    "\n",
    "### DPO의 핵심 개념\n",
    "- **선호도 학습**: 인간이 선호하는 답변 패턴 학습\n",
    "- **품질 차별화**: 좋은 답변과 나쁜 답변 구분\n",
    "- **가치 정렬**: 모델의 출력을 인간 가치에 맞게 조정\n",
    "\n",
    "### 실습 방법\n",
    "Distilabel을 이용해 선호도 데이터를 만드는 실습을 해보겠습니다.\n",
    "\n",
    "### Distilabel의 장점\n",
    "- **전문 도구**: DPO 데이터 생성에 특화\n",
    "- **자동화**: 수동 작업 대신 자동 생성\n",
    "- **품질 보장**: 고품질 선호도 데이터 생성\n",
    "- **효율성**: 빠르고 일관된 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APvasAo23Nui"
   },
   "source": [
    "### 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-sBfwMGI27Yj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 키가 입력되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드, 환경 변수에서 API 키 읽기\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "print(\"API 키가 입력되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pOARuBs3gRY"
   },
   "source": [
    "### Distilabel과 OpenAI API를 이용한 DPO 데이터 생성\n",
    "\n",
    "#### 접근 방법\n",
    "1. **다양한 프롬프트**: 여러 주제의 질문으로 데이터 다양성 확보\n",
    "2. **이중 모델 시스템**: \n",
    "   - 생성 모델: 좋은 답변과 나쁜 답변 생성\n",
    "   - 평가 모델: 답변 품질 평가 및 선호도 결정\n",
    "3. **파이프라인 구성**: 자동화된 데이터 생성 및 평가 과정\n",
    "\n",
    "#### 프롬프트 선택 기준\n",
    "- **다양성**: 다양한 주제와 난이도\n",
    "- **실용성**: 실제 사용자가 궁금해할 만한 질문\n",
    "- **평가 가능성**: 좋고 나쁨을 명확히 구분할 수 있는 주제\n",
    "\n",
    "#### 모델 설정 전략\n",
    "- **생성 모델 1**: 상대적으로 성능이 좋은 모델 (chosen 답변 생성)\n",
    "- **생성 모델 2**: 상대적으로 성능이 낮은 모델 (rejected 답변 생성)\n",
    "- **평가 모델**: 답변 품질을 평가하는 심판 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7H_9S5wB3GW0",
    "outputId": "53b9b935-b72d-4317-97df-b6dbded35995"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63/3699236350.py:2: DeprecationWarning: Importing from 'distilabel.llms' is deprecated and will be removed in a version 1.7.0. Import from 'distilabel.models' instead.\n",
      "  from distilabel.llms import OpenAILLM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from distilabel.llms import OpenAILLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps import FormatTextGenerationDPO, GroupColumns, LoadDataFromDicts\n",
    "from distilabel.steps.tasks import TextGeneration, UltraFeedback\n",
    "\n",
    "prompts = [\n",
    "    \"한국의 전통 시장을 배경으로 한 따뜻한 단편소설을 200자 내외로 작성해주세요.\",\n",
    "    \"중학생도 이해할 수 있도록 인공지능의 기본 개념을 설명해주세요.\",\n",
    "    \"재택근무와 사무실 근무의 장단점을 비교 분석해주세요.\",\n",
    "    \"기후변화의 주요 원인과 대응방안을 3가지씩 요약해주세요.\",\n",
    "    \"Python으로 리스트에서 중복을 제거하는 3가지 방법을 코드와 함께 설명해주세요.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 생성을 위한 모델 정의\n",
    "# 생성 모델 중 하나는 최신 모델, 다른 하나는 이전 세대 모델로 설정하여 성능 차이를 유도\n",
    "llm_generator_1 = OpenAILLM(\n",
    "    model=\"gpt-4.1-nano\",  # 상대적으로 성능이 좋은 모델\n",
    "    api_key=api_key,\n",
    "    generation_kwargs={\"max_new_tokens\": 500, \"temperature\": 0.3},\n",
    ")\n",
    "llm_generator_2 = OpenAILLM(\n",
    "    model=\"gpt-4o-mini\",  # 상대적으로 성능이 낮은 모델\n",
    "    api_key=api_key,\n",
    "    generation_kwargs={\"max_new_tokens\": 500, \"temperature\": 0.9},  # 높은 Temperature로 오답 생성 유도\n",
    ")\n",
    "\n",
    "# 생성된 답변을 평가할 모델 정의\n",
    "llm_judge = OpenAILLM(model=\"gpt-4.1-mini\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 생성 파이프라인 정의\n",
    "\n",
    "#### 파이프라인 구성 요소\n",
    "1. **데이터 로딩**: 초기 프롬프트 데이터 준비\n",
    "2. **답변 생성**: 두 개의 모델로 chosen/rejected 답변 생성\n",
    "3. **결과 그룹화**: 생성된 답변들을 하나로 통합\n",
    "4. **품질 평가**: 심판 모델로 답변 품질 평가\n",
    "5. **형식 변환**: 최종 DPO 학습 형식으로 변환\n",
    "\n",
    "#### 파이프라인 흐름\n",
    "프롬프트 → 답변 생성(2개 모델) → 그룹화 → 평가 → DPO 형식\n",
    "\n",
    "\n",
    "#### 각 단계의 역할\n",
    "- **LoadDataFromDicts**: 초기 데이터 로딩\n",
    "- **TextGeneration**: LLM을 통한 답변 생성\n",
    "- **GroupColumns**: 여러 결과를 하나로 통합\n",
    "- **UltraFeedback**: 답변 품질 평가\n",
    "- **FormatTextGenerationDPO**: 최종 DPO 형식 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 정의\n",
    "with Pipeline(name=\"dpo-generation-pipeline\") as pipeline:\n",
    "    initial_data = [{\"instruction\": p} for p in prompts]\n",
    "    load_prompts = LoadDataFromDicts(name=\"load_prompts\", data=initial_data)\n",
    "\n",
    "    # Chosen 답변 생성 (상세한 설명)\n",
    "    generate_responses_1 = TextGeneration(\n",
    "        name=\"generate_chosen\",\n",
    "        llm=llm_generator_1,\n",
    "        system_prompt=\"You are a friendly and verbose assistant that explains things in great detail.\",\n",
    "    )\n",
    "    # Rejected 답변 생성 (간단한 답변)\n",
    "    generate_responses_2 = TextGeneration(\n",
    "        name=\"generate_rejected\",\n",
    "        llm=llm_generator_2,\n",
    "        system_prompt=\"You are an assistant who only speaks about answers and omit all the process\",\n",
    "    )\n",
    "\n",
    "    # 생성된 답변들을 하나로 그룹화\n",
    "    group_responses = GroupColumns(\n",
    "        name=\"group_responses\",\n",
    "        columns=[\"generation\", \"model_name\"],\n",
    "        output_columns=[\"generations\", \"model_names\"],\n",
    "    )\n",
    "\n",
    "    # 심판 LLM으로 답변 품질 평가\n",
    "    evaluate_responses = UltraFeedback(\n",
    "        name=\"evaluate_responses\",\n",
    "        llm=llm_judge,\n",
    "        aspect=\"overall-rating\",\n",
    "    )\n",
    "\n",
    "    # DPO 학습 형식으로 변환\n",
    "    format_dpo = FormatTextGenerationDPO(name=\"format_dpo\")\n",
    "\n",
    "    # 파이프라인 연결\n",
    "    load_prompts.connect(generate_responses_1)\n",
    "    load_prompts.connect(generate_responses_2)\n",
    "    generate_responses_1.connect(group_responses)\n",
    "    generate_responses_2.connect(group_responses)\n",
    "    group_responses.connect(evaluate_responses)\n",
    "    evaluate_responses.connect(format_dpo)\n",
    "\n",
    "# 파이프라인 실행\n",
    "distiset = pipeline.run(use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 및 데이터프레임 변환\n",
    "\n",
    "#### 전처리 목적\n",
    "생성된 원시 데이터를 실제 DPO 훈련에 사용할 수 있는 형태로 변환합니다.\n",
    "\n",
    "#### 처리 과정\n",
    "1. **원시 데이터 확인**: Distilabel에서 생성된 데이터 구조 파악\n",
    "2. **필요 컬럼 추출**: instruction, chosen, rejected 컬럼만 선택\n",
    "3. **텍스트 정제**: assistant 역할의 답변 텍스트만 추출\n",
    "4. **데이터 검증**: 빈 값이나 오류 데이터 확인\n",
    "\n",
    "#### 최종 데이터 형태\n",
    "- **instruction**: 사용자 질문/지시사항\n",
    "- **chosen**: 선호되는 좋은 답변\n",
    "- **rejected**: 선호되지 않는 나쁜 답변\n",
    "\n",
    "#### 데이터 품질 확인\n",
    "- 답변의 완성도 검증\n",
    "- chosen과 rejected의 명확한 차이 확인\n",
    "- 데이터 형식의 일관성 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = distiset[\"default\"][\"train\"].to_pandas()\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_data = raw_df[[\"instruction\", \"chosen\", \"rejected\"]]\n",
    "# chosen과 rejected 컬럼에서 'assistant'의 답변 텍스트만 추출\n",
    "preference_data[\"chosen\"] = preference_data[\"chosen\"].apply(\n",
    "    lambda arr: next((item[\"content\"] for item in arr if item.get(\"role\") == \"assistant\"), None)\n",
    ")\n",
    "preference_data[\"rejected\"] = preference_data[\"rejected\"].apply(\n",
    "    lambda arr: next((item[\"content\"] for item in arr if item.get(\"role\") == \"assistant\"), None)\n",
    ")\n",
    "preference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15dKjH4B4YjW"
   },
   "source": [
    "## 마무리 및 내용 정리\n",
    "\n",
    "### 실습을 통해 배운 것들\n",
    "\n",
    "#### 1. 합성 데이터의 필요성\n",
    "- 고품질의 학습 데이터는 LLM 성능에 필수적\n",
    "- 데이터가 부족할 때 합성 데이터는 훌륭한 대안\n",
    "- 자동화된 데이터 생성으로 효율성 극대화\n",
    "\n",
    "#### 2. SFT 데이터 생성\n",
    "- LLM을 이용해 특정 주제에 대한 (지시, 응답) 쌍을 자동으로 생성\n",
    "- 모델이 지시를 잘 따르도록 만드는 방법 학습\n",
    "- 프롬프트 엔지니어링의 중요성 체험\n",
    "\n",
    "#### 3. DPO 데이터 생성\n",
    "- Distilabel을 활용한 자동화된 선호도 데이터 생성\n",
    "- 모델의 선호도를 인간의 가치에 맞게 정렬하는 방법\n",
    "- 파이프라인 기반의 체계적인 데이터 생성 과정\n",
    "\n",
    "### 실무 적용 방안\n",
    "이러한 기법들은 여러분이 LLM을 특정 목적에 맞게 고도화하고, 데이터 부족 문제를 해결하는 데 강력한 도구가 될 것입니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0182693b19d240cd9d21b343784f9323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c76958b6904458693276ca3b938a1c6",
      "placeholder": "​",
      "style": "IPY_MODEL_dedc6a0898514be597789176ccece224",
      "value": " 3/3 [01:53&lt;00:00, 34.61s/it]"
     }
    },
    "16f2cb1766ce40b1bfeff42e1a1ce4ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_568bb6c2aa5a482f91337aa2f9c38aba",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63753e9b3c144c5ab90ae3c8068dbaf1",
      "value": 3
     }
    },
    "27c861d6e5454f77a5915f256f5fce58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "568bb6c2aa5a482f91337aa2f9c38aba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f47ea24d18b4ee0961999f233109282": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cdd0fa5a63ab461399c7aecd2a15d575",
       "IPY_MODEL_16f2cb1766ce40b1bfeff42e1a1ce4ac",
       "IPY_MODEL_0182693b19d240cd9d21b343784f9323"
      ],
      "layout": "IPY_MODEL_bcf677fba02b4fd79d96271c17bdae35"
     }
    },
    "63753e9b3c144c5ab90ae3c8068dbaf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c76958b6904458693276ca3b938a1c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95a374b0370d493da512b8d9871582df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bcf677fba02b4fd79d96271c17bdae35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdd0fa5a63ab461399c7aecd2a15d575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27c861d6e5454f77a5915f256f5fce58",
      "placeholder": "​",
      "style": "IPY_MODEL_95a374b0370d493da512b8d9871582df",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "dedc6a0898514be597789176ccece224": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
